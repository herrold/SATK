#!/usr/bin/python3.3
# Copyright (C) 2013 Harold Grovesteen
#
# This file is part of SATK.
#
#     SATK is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     SATK is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with SATK.  If not, see <http://www.gnu.org/licenses/>.

# The module provides prepackaged use of the foundation language tools.  The 
# classes provided by this module fall into two categories:
#
#   1. Smart Tokens - subclasses of lexer.Type and lexer.Token providing prepackaged
#                     support for commonly occurring types of data.
#   2. Key-Word Language - A class that allows easy definition of a language
#                     using keyword-value pairs associated with statements. 
#                     Internally a lexer and parser will be built a
#
# Descriptions of the facilities provided herein accompany the object definitions.
#
# This module is intended to be imported for use by other modules for the purpose
# of reusing developed language components.  Refer to the document "SATK for s390 -
# Language Processing Tools" in the SATK doc directory for a description of how 
# the base components are used in this module

# Python imports: None

# SATK imports:
import lang
import lexer
import parser

# +----------------+
# |  Smart Tokens  |
# *----------------+

# Smart tokens understand a specific character encoding of certain frequently 
# encountered data types.  They also provide a mechanism for conversion of the
# recognized string data into a Python data type.  Each smart token is defined
# by a subclass of lexer.Type that recognizes the data and a subclass of 
# lexer.Token that provides the conversion mechanism for use by semantic analysis.
# A token id must be supplied with each smart token type recognizer.
#
# The following smart tokens are supported:
#
#   Type      Token   Default
# subclass   subclass   TID     Description
#
#  BinType   BinToken   BIN   Recognizes binary data: 0b0101 or B'0101'
#  CmtType   CmtToken   CMT   Recognizes comments.
#  DecType   DecToken   DEC   Recognizes decimal data: 500, or +500 or -500
#  HexType   HexToken   HEX   Recognizes hexadecimal data: 0x535  or X'535'
#                             Various combinations of encoding and alpha case are 
#                             supported.
#  NamType   NamToken   NAME  Recognizes names of alphabetic, numeric and special
#                             characters.
#  NLType    NLToken    NL    Platform independently recognizes new lines.
#  StgType   StgToken   STG   Recognizes storage sizes with units: 
#                             B, K, M, G, T, P or E.
#  StrType   StrToken   STR   Recognizes single, double quoted strings or both
#  WordType  WordToken  none  Recognizes a one or more strings as part of the type
#  WsType    WsToken    WS    Recognizes white space

# The token generated by DecType.  
# The convert() method returns an integer from recognized string
class BinToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return int(self.string,2)

# This token type recognizes a binary strings.  The binary string may be 
# coded using either the:
#   - GNU as convention.  A '0b' precedes the binary digits.  For example: 0b1011
#   - ASM conventions.  A "B'" precedes the binary digits which are followed by
#     a closing singe quote.  For example: B'1011'.
#
# Which format(s) are recognized is specified by setting the format argument:
#   - 'b' - GNU as and ASM conventions.  (This is the default)
#   - 'g' - GNU as convention only
#   - 'i' - ASM convention only
class BinType(lexer.Type):
    exp={"g":r"0b[01]+",
         "i":r"B'[01]+'",
         "b":r"0x[01]+|X'[01]+'"}
    def __init__(self,tid="BIN",format="b",debug=False):
        try:
            exp=BinType.exp[format]
        except KeyError:
            raise ValueError("langutil.py - BinType - invalid format: '%s'" % format)
        super().__init__(tid,exp,tcls=BinToken,debug=debug)

# The token generated by CmtType.  
# The convert() method returns the recognized comment (if needed).
class CmtToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return self.string

# This method defines a token type that recognizes comments starting with a special
# character.  The character is specified by the char argument.  It defaults to
# a '#' character.
class CmtType(lexer.Type):
    def __init__(self,tid="CMT",char="#",ignore=True,debug=False):
        exp="%s%s" % (char[0],r"[^\n]*")
        super().__init__(tid,exp,tcls=CmtToken,ignore=ignore,debug=debug)

# The token generated by DecType.  
# The convert() method returns an integer from recognized string
class DecToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return int(self.string,10)

# This method defines a token Type that recognizes signed/unsigned decimal input
class DecType(lexer.Type):
    def __init__(self,tid="DEC",debug=False):
        super().__init__(tid,r"[\+-][0-9]+",tcls=DecToken,debug=debug)

# The token generated by HexType.
# The convert() method returns an integer from recognized string
class HexToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        if self.string[:2]=="X'":
            hexdigits=self.string[2:-1]  # Drop the beginning "X'" and ending "'"
        else:
            hexdigits=self.string[2:]    # Drop the beginning "0x"
        return int(hexdigits,16)

# This token type recognizes a hexadecimal strings.  The hexadecimal string may be 
# coded using either the:
#   - GNU as convention.  A '0x' precedes the hexadicimal digits.  For example: 0x5F
#   - ASM conventions.  A "X'" precedes the hexadecimal digits which are followed by
#     a closing singe quote.  For example: X'5F'.
# The alpha hexadecimal characters may be coded using upper or lower case.
#
# Which formats are recognized is specified by setting the format argument: 
#   - 'gl' - GNU as lower case alpha (This is the typical GNU as encoding, like C)
#   - 'il' - ASM lower case alpha
#   - 'gm' - GNU as with either lower or upper case alpha
#   - 'im' - ASM with either lower or upper case
#   - 'gu' - GNU as upper case alpha
#   - 'iu' - ASM upper case (This is the typical ASM encoding)
#   - 'bl' - GNU as and ASM with lower case alpha
#   - 'bm' - GNU as and ASM with lower or upper case alpha (This is the default)
#   - 'bu' - GNU as and ASM with upper case alpha
#   - 'bt' - Recognizes GNU as with lower case and ASM with upper case alpha
class HexType(lexer.Type):
    exp={"gl":r"0x[0-9a-f]+",
         "il":r"X'[0-9a-f]+'",
         "gm":r"0x[0-9a-fA-F]+",
         "im":r"X'[0-9a-fA-F]+'",
         "gu":r"0x[0-9A-F]+",
         "iu":r"X'[0-9A-F]+'",
         "bl":r"0x[0-9a-f]+|X'[0-9a-f]+'",
         "bm":r"0x[0-9a-fA-F]+|X'[0-9a-fA-F]+'",
         "bu":r"0x[0-9A-F]+|X'[0-9A-F]+'",
         "bt":r"0x[0-9a-f]+|X'[0-9A-F]+'"}
    def __init__(self,tid="HEX",format="bm",debug=False):
        try:
            exp=HexType.exp[format]
        except KeyError:
            raise ValueError("langutil.py - HexType - invalid format: '%s'" % format)
        super().__init__(tid,exp,tcls=HexToken,debug=debug)

# The token generated by NamType.
# The convert() method returns an the name as a string
class NamToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return self.string

# This class recognizes names starting with an alphabetic character followed by 
# any number of alphabetic characters, number, 0 through 9.  Various environments
# allow additional characters to be used within a name, for example an underscore,
# '_', or dollar sign, '$'.  These additional characters are specified as a string
# argument to the instance argument 'special'.
class NamType(lexer.Type):
    def __init__(self,tid="NAME",special="",debug=False):
        if not isinstance(special,str):
            raise ValueError("langutil.py - NamType - requires 'special' to be a"
                "string: %s" % special)
        exp="[a-zA-Z][a-zA-Z0-9%s]*" % special
        super().__init__(tid,exp,tcls=NamToken,debug=debug)
        
# The token generated by NLType.
# The convert() method returns a string containing the number of recognized new
# lines.
class NLToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return self.string
        
# This class recognizes new lines.  Because is uses Python's '\n' designation,
# new line recognition is platform independent.
class NLType(lexer.Type):
    def __init__(self,tid="NL",ignore=True,debug=False):
        super().__init__(\
            tid,r"(?m)\n+",eol=True,tcls=NLToken,ignore=ignore,debug=debug)

# The token generated by StgType.
# The convert() method returns an integer for the recognized storage quantity and
# units in bytes.
class StgToken(lexer.Token):
    units={"B":1,
           "K":1024,
           "M":1048576,
           "G":1073741824,
           "T":1099511627776,
           "P":1125899906842624,
           "E":1152921504606846976}
    def __init__(self):
        super().__init__()
    def convert(self):
        unit=self.string[-1]
        try:
            unit=StgToken[unit]
        except KeyError:
            raise ValueError("langutil.py - StgToken - internal error: StgType "
                "accepted a unit not recognized by StgToken: %s" % unit)
        quantity=int(self.string[:-1],10)
        return unit*quantity

# This class recognizes storage size designations with these size units:
#   B  =  1 byte
#   K  =  1,024 bytes
#   M  =  1,048,576 bytes or 1024K
#   G  =  1,073,741,824 bytes or 1024M
#   T  =  1,099,511,627,776 bytes or 1024G
#   P  =  1,125,899,906,842,624 bytes or 1024T
#   E  =  1,152,921,504,606,846,976 bytes or 1024P
#
# Class instance arguments:
#   tid     The token id used by this token type
#   units   A string containing the valid units recognized by the token.
#           By default all units defined above are accepted.  If explictly specified
#           they may be in any order.
# Note: An instance of this type should be defined by to a lexer before an instance 
# of DecType class.  Otherwise DecType will recognize the numeric portion of the
# storage size string and leave the unit as unrecognized or recognized as another
# token type.
class StgType(lexer.Type):
    def __init__(self,tid="STG",units="BKMGTPE",debug=False):
        if not isinstance(units,str):
            raise ValueError("langutil.py - StgType - requires 'units' to be a"
                "string: %s" % units)
        for x in range(len(units)):
            u=units[x]
            if u not in ["B","K","M","G","T","P","E"]:
                raise ValueError("langutil.py - StgType - invalid unit at position "
                    "%s: %s" % (x+1,u))
        exp="[0-9]+[%s]" % units
        super().__init__(tid,exp,tcls=StgToken,debug=debug)

# The token generated by StrType.
# The convert() method returns the string without enclosing quotes
class StrToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return self.string[1:-1]
        
# This class recognizes single and/or double quoted strings.
# The type of enclosing quotes is specified by the quotes argument.
#   'd' - Recognizes strings enclosed by double quotes: "a string"
#   's' - Recognizes strings enclosed by single quotes: 'a string'
#   'sd' or 'ds' - Recognizes strings enclosed by either single or double quotes.
# Default quotes values is 'd'.
class StrType(lexer.Type):
    double='"[^"]*"'
    single="'[^']*'"
    def __init__(self,tid="STR",quotes="d",debug=False):
        if not isinstance(quotes,str):
            raise ValueError("langutil.py - StrType - requires 'quotes' to be a"
                "string: %s" % quotes)
        single=False
        double=False
        for x in range(len(quotes)):
            q=quotes[x]
            if q == "d":
                double=True
            elif q == "s":
                single=True
            else:
                raise ValueError("langutil.py - StrType - invalid quote at position "
                    "%s: %s" % (x+1,q))
        exp=[]
        if single:
            exp.append(StrType.single)
        if double:
            exp.append(StrType.double)
        exp="|".join(exp)
        super().__init__(tid,exp,tcls=StrToken,debug=debug)

# The token generated by WordType.
# The convert() method returns the recognized word.
class WordToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return self.string

# This class recognizes a series of words as part of a specific type.  This type
# allows multiple words to grouped together as a single type.  Words are specified
# in a list of strings. The first two characters of the string define the 
# recognition case sensitivity and the remainder of the string identifies the
# characters recognized by the string.
#
# The following case sensitivities are allowed:
#   L:  - only lower case recognized
#   U:  - only upper case recognized
#   E:  - either all upper case or all lower case, but not mixed, is recognized
#   M:  - mixed case is recognized
#  If case sentivity is not provided the word is recognized as supplied.
#
# Instance arguments:
#   tid    Token type id (required).
#   words  The list of word specifiers
class WordType(lexer.Type):
    def __init__(self,tid,words=[],debug=False):
        if not isinstance(words,list):
            wds=[words,]
        else:
            wds=words

        # Dictionary of methods corresponding to case sensitivity
        redict={"E:":self._either,
                "L:":self._lower,
                "M:":self._mixed,
                "U:":self._upper}
                
        # dictionary of input words to regular expressions       
        expr={}
        for x in range(len(wds)):
            w=wds[x]
            if not isinstance(w,str):
                raise ValueError("langutil.py - WordType - words[%s] not a string:"
                    " %s" % (x,w))
            case=w[:min(len(w),2)]
            try:
                m=redict[case]
                expr[w]=m(w[2:])
            except KeyError:   
                expr[w]=w
                
        # Sort the words so that the greediest word is recognized first.
        strings=[]
        for w in expr:
            strings.append(w)
        order=sorted(strings,reverse=True)
        # Build a list of regular expressions in recognition order.
        exp=[]
        for x in order:
            exp.append(expr[x])
        # Join the individual expressions into one big one
        exp="|".join(exp)
        super().__init__(tid,exp,tcls=WordToken,debug=debug)

    # Return a regular expression recognizing all upper or all lower case
    def _either(self,word):
        return "%s|%s" % (word.upper(),word.lower())

    # Return a regular expression recognizing the letters in lower case
    def _lower(self,word):
        return word.lower()

    # Return a regular expression recognizing the letters in mixed case
    def _mixed(self,word):
        exp=""
        for x in range(len(word)):
            letter=word[x]
            if letter.islower():
                exp="%s[%s%s]" % (exp,letter.upper(),letter)
                continue
            if letter.isupper():
                exp="%s[%s%s]" % (exp,letter,letter.lower())
                continue
            exp="%s%s" % (exp,letter)
        return exp
        
    # Return a regular expression recognizing the word in upper case
    def _upper(self,word):
        return word.upper()

# The token generated by WsType.
# The convert() method returns the recognized white space 
class WsToken(lexer.Token):
    def __init__(self):
        super().__init__()
    def convert(self):
        return self.string

# This class recognizes white space: spaces, vertical or horizontal tabs, form feed,
# or carriage return.
# Note: because the regular expression does not include the MULTILINE option, the
# linefeed sequence is not recognized by the WsType.  Only the NLType class 
# recognizes the linefeed.
class WsType(lexer.Type):
    def __init__(self,tid="WS",ignore=True,debug=False):
        super().__init__(tid,r"[ \t\r\f\v]+",tcls=WsToken,ignore=ignore,\
            debug=debug)

# +-------------------------------+
# |  Key-Word Language Processor  |
# *-------------------------------+

# The Key-Word Language Processor is wrapper class encapsulating a lexical and
# syntactic analyzer for languages containing statements of positional and key
# word value pairs.  It uses its own subclasses of lexer.Lexer and parser.Parser
# for language recognition.  A specific instance of this language will subclass
# the base class, KWLang, and initialize it via the KWLang subclass' init() method.
# This is the same technique used by other classes of the SATK language tools.
#
# These classes are provided for the purpose of building a key-word language
# processor.
#
#    KWDef        An instance of this class defines a statement and its arguments
#    KWKeyWord    An instance of this class defines a statement's keyword argument
#    KWLang       The base class used to implement a Key-Word language by a 
#                 subclass.  It is a subclass of lang.Lanuage and parser.Parser.
#    KWStatement  An instance of this object is returned for each successfully
#                 recognized statement.

# This is the lexer built internally by KWLang.  It is not referenced externally.
class _KWLexer(lexer.Lexer):
    def __init__(self):
        super().__init__()
    def init(self,types):
        # Register all of the supplied types
        for x in types:
            self.type(x)
        return self

# The KWDef class defines a statement recognized by KWLang.  The statement object is
# defined using instances of tokens.  Token ID's must be in upper case. Once the 
# statement object is instantiated, positional and keyword arguments are
# added by calling instance methods.
#
# The grammar that is produced recognizes this basic structure statement structure:
#
#   <statement_name>  <positional_arguments>  <keyword_pairs>
#
# Positional arguments are recognized by their token id's.  They may be coded in
# any sequence.
#
# The only required component of a statement is its initial identifier.  
# It is possible to define statements without any arguments.
#
# Instance arguments:
#   name      This argument defines and identifies the statement.  It requires a
#             tid of an instance of WordType registered with KWLang.
#   required  Specify True is this statement is required.  Specify False if not.
#             Default is False
#   multiple  Specify True if multiple statements are allow.  Specify False if
#             only one is allowed.  Defaults to False.
#
# Instance methods:
# They following methods define arguments of the statement.
#
#  keyword()      Adds KWKeyWord instances defining statement keyword/value pair
#  positional()   Adds required positional arguments to the statement.   
class KWDef(object):
    @staticmethod
    # This method converts a smart token or regular token into its value.
    # For smart tokens, the convert method() is used.  For base token's its string
    # attribute is returned.
    def tok2val(token):
        try:
            return token.convert()
        except AttributeError:
            pass
        return token.string
    def __init__(self,name,required=False,multiple=False):
        if not isinstance(name,str):
            raise ValueError("langutil.py - KWDef - 'name' argument must be a "
                "string: %s" % name)

        self.required=required
        self.multiple=multiple
        # All of the TID's will checked by the _validate() method.  
        self.tid=name       # TID of WordType instance recognizing this statement
        self.positionals=[] # TID's of required positional arguments
        self.num_pos=0      # Number of required positional arguments

        self.keywords=[]    # _KWKeyWord instance list defining keyword arguments
        self.keywordsd={}   # _KWKeyWord instance dictionary by keyword 
        self.kwd_tids=[]    # List of key word recognizing TID's
        self.kwd_cbs=[]     # Key-word call backs
        self.pos_cbs=[]     # Positional call backs

        # These values will be established by the _validate() method
        self.name=None      # The name of the statement (same as pid)
        self.pid=None       # The statements PID in lower case     

    # Create the grammar statements for this statement
    def _init_grammar(self):
        # This method creates grammar statements from the following template:
        #  <stmt_pid> -> STMT_TID <stmt_pid._pos> <stmt_pid._key>*
        #                             pos_pid          key_pid
        #  <stmt_pid._pos>  -> POS_TID1 POS_TID2 ...
        #  <stmt_pid._key>  -> key_pid1  # defer for now <!resync with KEY_TIDs>
        #  <stmt_pid._key>  -> key_pid2
        #    etc...
        #  [ See KWKeyWord._init_grammar() for key word generated statements
        #    etc...
        
        #key_pid=""
        #key_pids=""
        #if len(self.keywords)>0:
        #    key_pid="%s_key"   % self.pid
        #    key_pids="%s_key*" % self.pid
        
        g="\n# Productions for Statement: %s\n" % self.pid
        
        #  BUILD: <stmt_pid> -> STMT_TID <stmt_pid._pos> <stmt_pid._key>*
        positionals=len(self.positionals)
        keywords=len(self.keywords)
        stmtNT=self.pid
        stmtT=self.tid
        
        posNTs=keyNTs=posNT=keyNT=""
        if positionals:
            posNT="%s_pos" % self.pid
            posNTs=" %s" % posNT
            self.pos_cbs.append(posNT)
            
        if keywords:
            if keywords==1:
                keyNT="%s_key" % self.pid
                keyNTs=" %s" % keyNT
            else:
                keyNT=" %s_key*" % self.pid
            
        g="%s%s -> %s%s%s\n" % (g,stmtNT,stmtT,posNTs,keyNTs)
        
        # BUILD <stmt_pid._pos> -> posTID1 posTIS2 ...
        if positionals:
            posTIDs=" ".join(self.positionals)
            g="%s%s -> %s\n" % (g,posNT,posTIDs)
        
        # Build keyword related productions
        if keywords:
            cbs=[]
            # BUILD <stmt_pid._key> -> keyPID 
            #          etc.
            for x in self.keywords:
                keywdNT=x.pid
                self.kwd_cbs.append(x.pid)
                g="%s%s -> %s\n" % (g,keyNT,keywdNT)
            for x in self.keywords:
                # Add: keyPID -> keyTID EQUAL keyPID._args
                #      keyPID._args -> key_arg_TID
                #         etc.
                g="%s%s" % (g,x._init_grammar())
        return g

    # This method sets keyword argument defaults and returns a list of 
    # required keyword arguments that are missing.  KWLang will report on
    # the missing required arguments.
    def _set_default(self,statement):
        required=[]
        if not isinstance(statement,KWStatement):
            raise ValueError("langutil.py - KWDef._setdefault() - 'statement' "
                "must be an instance of KWStatement: %s" % statement)
        required=[]
        for x in self.keywords:
            keyword=x.pid
            default=x.default
            if default is None:
                try:
                    statement[keyword]
                except KeyError:
                    required.append(keyword)
                continue
            statement._set_default(x.pid,default)
        return required

    # Attepmts to set the keyword/argument value in the resulting KWStatement
    # instance.  It returns the result from the KWStatement object (which knows
    # the current state of recognized keywords in the source text).
    # Returns:
    #   True   If able to set the keyword argument or add it to the list of
    #          multiple occurrences of the keyword.
    #   False  If the keyword defined with 'multiple=False' is attempted to be
    #          set more than once.
    def _set_keyword(self,statement,key,key_tok,arg_tok):
        try:
            keywd=self.keywordsd[key]
        except KeyError:
            raise ValueError("langutil.py - KWDef._setkeyword() - 'key' argument "
                "not a defined keyword for statement '%s': '%s'" \
                % (self.tid,key))
        #try:
        #    value=arg_tok.convert()
        #except AttributeError:
        #    value=arg_tok.string
        value=KWDef.tok2val(arg_tok)
        res=statement._set_keyword(key,value,key_tok,arg_tok,lst=keywd.multiple)

    # This method sets the positional values in a statement
    # Returns the number of missing required tokens.  Zero implies success
    def _set_positionals(self,statement,pos_tokens):
        num_tok=len(pos_tokens)
        num_req=len(self.positionals)
        missing=0
        if num_tok!=num_req:
            return num_req-num_tok
        # All of the required tokens were found
        pos=[]
        for x in pos_tokens:
            pos.append(KWDef.tok2val(x))
        statement._set_positionals(pos,pos_tokens)

    # This method is called by LKWLang when the KWDef instance is added to the 
    # the language processor in KWLang.statement() method.  All TID's are converted
    # to actual instances and checked for valid instance
    def _validate(self,kwl):
        token=kwl._getToken(self.tid)
        if token is None:
            raise ValueError("langutil.py - KWDef - 'name' TID not registered: "
                "%s" % self.tid)
        if not isinstance(token,WordType):
            raise ValueError("langutil.py - KWDef - 'name' argument TID must be an "
                "instance of WordType: %s" % token)
        self.pid=self.tid.lower()
        self.name=self.pid

        for x in self.positionals:
            tok=kwl._getToken(x)
            if tok is None:
                raise ValueError("langutil.py - KWDef - required argument TID not"
                    " registered: %s" % x)

        for x in self.keywords:
            x._validate(kwl)  # Validate the keyword definitions

    # This method adds a keyword definitions to the statement.
    # Method arguments:
    #   kwd      A list of KWKeyWord instances defining how to recognize the
    #            keywords/value pair.
    def keyword(self,kwd=[]):
        if not isinstance(kwd,list):
            k=[kwd,]
        else:
            k=kwd
        for x in range(len(k)):
            kwd=k[x]
            if not isinstance(kwd,KWKeyWord):
                raise ValueError("langutil.py - KWDef.keyword() - 'kwd[%s]' not "
                    "an instance of KWKeyWord: %s" % (x,kwd))
            ktok=kwd.token.lower()

            try:
                ko=self.keywordsd[ktok]
                raise ValueError("langutil.py - KWDef.keyword() - duplicate "
                    "keyword arguments: '%s'" % ktok)
            except KeyError:
                self.keywordsd[ktok]=kwd         # Add it by its TID name
                self.keywords.append( kwd )
                self.kwd_tids.append( ktok )     # Remember TID of keyword

    # This method adds required positional arguments to the statement.  Positional
    # statement arguments are identified by their position, not by their TID.
    #
    # Because TID's are unique and tied directly to the regular expression that
    # recognized the TID, they same sequence, for example, a name, will be
    # associated the same TID is all occurrences.  So it would become impossible
    # to recognize by the TID of a name which positional argument the name refers
    # other than by position.  This then forces all positional arguments to be
    # required.
    #
    # Method arguments:
    #    tokens   This is a list of registered TID's instances that recognize 
    #             required positional arguments.
    def positional(self,tokens=[]):
        if not isinstance(tokens,list):
            tids=[tokens,]
        else:
            tids=tokens
        for x in range(len(tids)):
            tid=tids[x]
            if not isinstance(tid,str):
                raise ValueError("languitil.py - KWDef.positional() - 'tokens[%s]'"
                    " must be a string: %s" % (x,tid))
        self.positionals.extend(tids)
        self.num_pos+=len(tids)

# This class defines a keyword/value pair.
# Instance arguments:
#   token     A TID corresponding to the registered token that recognizes the
#             keyword.  Corresponding TID must be an instance of WordType
#   argument  A list of registered token type TID' that identify the keyword's 
#             argument.  Any TID corresponding to an instance of lexer.Type
#             accepted, including smart tokens.
#   multiple  Specifies whether the keyword may occur multiple times in the
#             statement.  Specify 'False' for only one occurence.  Specify 'True' 
#             if multiple instances of the argument are allowed.  Semantic 
#             processing by KWLang recognizes this condition.  The generated
#             grammar allows all keyword value pairs to be coded multiple
#             times.
#   default   Specify a default value for the keyword argument.  If not 
#             specified, the keyword is required.  Semantic processing by 
#             KWLang recognizes this condition, not the generated grammar.
class KWKeyWord(object):
    def __init__(self,token,arguments=[],multiple=False,default=None):
        self.token=None          # TID of keyword recognizing token type
        self.arguments=[]        # TID list of recognizing argument values
        self.default=default     # If specified the native default value
        self.multiple=multiple   # Multiple occurrences allowed

        # These values are established by the _validate() method
        self.pid=None            # The keyword's production PID
        self.name=None           # The keyword's name (same as PID)

        if not isinstance(token,str):
            raise ValueError("langutil.py - KWKeyWd - 'token' argument must be "
                "a string: %s" % token)
        self.token=token

        if not isinstance(arguments,list): 
            args=[arguments,]
        else:
            args=arguments
        for x in range(len(args)):
            arg=args[x]
            if not isinstance(arg,str):
                raise ValueError("langutil.py - KWKeyWd - 'arguments[%s]' "
                    "must be a string: %s" % arg)
        self.arguments=args
        
    # Returns a tuple: ("grammar statements",<key_pid>)
    def _init_grammar(self):
        # builds the grammar for the keyword using this template
        # <key_pid> -> <KEY_TID> EQ <key_pid._args>
        # <key_pid._args> -> ARG_TID1
        # <kwy_pid._args> -> ARG_TID2
        #   etc...
        if len(self.arguments)==0:
            raise ValueError("langutil.py - KWKeyWord._init_grammar() - internal "
                "error, self.arguments empty, can not build grammar: %s" \
                % self.name)
        
        key_args="%s_args" % self.pid
        keyword="%s -> %s EQUAL %s\n" % (self.pid,self.token,key_args)
        for x in self.arguments:
            keyword="%s%s -> %s\n" % (keyword,key_args,x)
        return keyword

    def _validate(self,kwl):
        tok=kwl._getToken(self.token)
        if tok is None:
            raise ValueError("langutil.py - KWKeyWd - 'token' argument not "
                "registered: %s" % self.token)
        if not isinstance(tok,WordType):
            raise ValueError("langutil.py - KWKeyWd - 'token' argument must be "
                "and instance of WordType: %s" % tok)
        self.pid=self.token.lower()
        self.name=self.pid

        for x in range(len(self.arguments)):
            tid=self.arguments[x]
            tok=kwl._getToken(tid)
            if tok is None:
                raise ValueError("langutil.py - KWKeyWd - 'arguments[%s]' not " 
                    "registered: %s" % (x,tid))

# This class allows a subclass to create and process a key-word oriented language.
# The subclass registers the token types it has of interest.  These TID's are
# used to build the recognizing grammar and basic semantic processing.  All tokens
# should be registered before being referenced by a statement or keyword 
# definition.
#
# Instance arguments: None
#
# KWLang instance methods used by a KWLang subclass:
#   token          Defines registered token types by TID.  TID's must be upper
#                  case.
#   statements     Defines a statement to the language using KWDef instances
#   init_lang      Method used by a subclass to initialize language processing
#                  after all token types have been registerd and statements 
#                  have been defined.  The keyword language processor is now ready
#                  for use of the methods described next.
#
# lang.Processor instance methods used by a KWLang subclass
#
#   errors         Returns the number of currently encountered errors.
#
# KWLang instance methods used by a KWLang subclass or user of the subclass itself.
# This allows language processing to be controlled by
#   - either the specific instance of a keyword language or
#   - instantiator of the keyword language.
#
#   analyze        Parses and processes text statements.  Results in a list of
#                  KWStatement instances returned to the method caller for
#                  additional semantic processing.
#   erreport       Produce the consolidated error report.
#   semerror       Report semantic errors to the error manager for consolidated 
#                  reporting.
#
# KWLang is a subclass of lang.Processor.
#
#  Methods supplied by KWLang for lang.Processor:
#       Method        Description
#    create_lexer   returns an instance of lexer.Lexer with registered token types.
#                   Token types are registered by the KWLang subclass by using the
#                   KWLang.token() method before calling KWLang.init_lang()
#    define_parser  Returns a tuple of the language grammar and starting PID.
#                   KWLang builds the grammar based upon statements registered with
#                   KWLang by the subclass using the KWLang.statements() method
#                   before calling KWLang.init_lang().
#
#  Methods supplied by KWLang for parser.Parser:
#     init          Enables debugging options for parser.  And establishes 
#                   KWLang call back methods.
#
class KWLang(lang.Processor):
    def __init__(self):
        super().__init__()
        # Lexical analysis related attributes used to create _KWLexer instance
        self._tokens={}   # Dictionary of registered token ID's
        # This is the sequence in which tokens will be recognized by the lexer.
        # The _validate() methods ensure the TID's defined in KWDef and _KWKeyWd
        # are will be reconized by the lexer and are of the type expected by the
        # definitions.
        self._lexlist=[]  # lexer tokens

        # Syntactical analysis related attributes used to create _KWParser instance.
        self._stmts=[]       # The list of validated statements (KWDef instances)
        self._stmtsd={}      # Dictionary of defined statements (KWDef instnaces)
        self._stmts_pids=[]  # List of statement PID's (and stmt call backs)
        self._stmts_sync=[]  # List of statement resync TID's
        self._pos_cbs=[]     # List of positional call backs
        self._kwd_cbs=[]     # List of keyword call backs
        self._kwd_arg_cbs=[] # List of keyword argument call backs
        
        # Language processing performed by basic tools
        self._grammar=None
        self._lexer=None
        self._parser=None
        self.gs=None      # Returned by lang.Language.analyze
        
        # Statements (KWStatement instances) recognized by language processing
        self._statements=[]   # See cb_statement_end() method
        self._stmt_counts={}  # Number of occurrences of the statement

    # Initialize the grammar kew-word language syntactic processing.
    def __init_grammar(self,gdebug=False):
        # The following grammar statements are generated with this template
        #   start -> statement*
        #   statement  -> stmt_pid1  <!resync with STMT_TIDs>
        #   statement  -> stmt_pid2
        #     etc...
        # The KWDef._init_grammar() method generates remaining statements
        if len(self._stmts)==0:
            raise ValueError("langutil.py - KWLang._init_grammar() - can not "
                "generate key-word language grammar, no statement defined")

        g="# %s Key-Word Language Grammar\n\n" % self.__class__.__name__
        g="%s%s" % (g,"start -> statements*\n")

        # Generate each 'statement' alternative
        resync=""
        #RESYNC deferred until basic grammar is debugged
        #for tid in self._stmts_sync:
        #    resync=" %s <!%s>" % (resync,tid)
        stmt=""
        for stmtNT in self._stmts_pids:
            stmt="statements -> %s%s\n" % (stmtNT,resync)
            resync=""  # Only put resync tokens on first production altenative

        # Add the remaining productions for each statement
        g="%s%s" % (g,stmt)
        for x in self._stmts:
            g="%s%s" % (g,x._init_grammar())
            self._pos_cbs.extend(x.pos_cbs)
            cbs=x.kwd_cbs
            self._kwd_cbs.extend(cbs)
            for x in cbs:
                self._kwd_arg_cbs.append("%s_args" % x)
        self._grammar=g
        if gdebug:
            print("Generated Grammar:\n%s" % self._grammar)

    # Initializes the lexer
    def __init_lexer(self,kdebug=False,tdebug=False):
        self.token(WordType("EQUAL",words="=",debug=tdebug))
        self.token(WsType(debug=tdebug))
        self.token(CmtType(debug=tdebug))
        self.token(NLType(debug=tdebug))
        #self.token(lexer.EOSType(debug=tdebug))
        self._lexer=_KWLexer().init(self._lexlist)
        if kdebug:
            self._lexer.types()
            
    # Extract token errors from error list
    def __find_token_errors(self,eo):
        lst=[]
        for x in eo:
            if isinstance(x,parser.ErrorToken):
                lst.append(x)
        if self.lang.isdebug("edebug"):
            print("langutil.py - KWLang.__find_token_errors returning: %s" % lst)
        return lst
            
    # Find token errors in error list
    def __recognize_new_token_errors(self,te,eo):
        lst=self.__find_token_errors(eo)
        add=[]
        for x in te:
            for y in lst:
                if parser.Error.compare(y,x)==0:
                    continue
                add.append(y)
        if self.lang.isdebug("edebug"):
            print("langutil.py - KWLang.__recognize_new_token_errors - "
                "extending error list with: %s" % add)
        te.extend(add)
    
    # Check for required statements at the end of the process
    def __required_statements(self):
        for x in self._stmts:
            if not x.required:
                continue
            # Statement _is_ required
            name=x.name
            try:
                self._stmt_counts[name]
                # found at least one so we are good to go.
            except KeyError:
                message="required statement missing: '%s'" % name
                self.semerror("__required_statements",\
                    type="",token=None,message=message)
    
    # Reset the state of argument processing
    def __reset_argument(self,gs):
        gs.argument_token=None    # Set by cb_argument_token()
        # Token errors added by cb_argument_error()
        gs.argument_eos=[]        # Error objects while recognizing argument 
        return True

    # Reset the state of keyword processing
    def __reset_keyword(self,gs):
        gs.keyword=None           # Set by cb_keyword_beg()
        gs.kwo=None               # KWKeyWord object of the keyword
        gs.keyword_state=0
        gs.keyword_token=None     # Set by cb_keyword_token()
        gs.keyword_argument=None  # Token set by cb_argument_end()
        # Token errors added by cb_keyword_error()
        gs.keyword_eos=[]         # Error objects while recognizing keyword
        gs.keyword_equal_eos=[]   # Expected '=' but found this error
        return True

    # Reset the state of keyword processing
    def __reset_positional(self,gs):
        gs.positionals=[]         # Tokens added to it by cb_positional_token()
        # Token errors added by cb_positional_error()
        gs.positional_eos=[]      # Error objects while recognizing positional
        return True
         
    # Reset the state of statement processing
    def __reset_statement(self,gs):
        # These two attributes are set by cb_statement_beg()
        gs.stmt_def=None         # KWDef instance of the statement being built
        gs.statement=None        # KWStatement instance being built by call backs
        # This attribute is set by cb_statement_token()
        gs.statement_token=None  # Token recognizing start of the statement
        # Token errors added by cb_statement_failing()
        gs.statement_eos=[]      # Error objects while recognizing positional
        return True
        
    # This method tracks recognized statement results and reports on 
    # when more than one statement is recognized for statements that may not
    # be provided more than once
    def __statement_result(self,stmt,multiple):
        # I am going to add another statement to the results.
        try:
            counts=self._stmt_counts[stmt.name]
            # this means we have at least one of these before
            if not multiple:
                # OOPS, can't have more than one, so generate an error
                message="'%s' statement may only occur once" % stmt.name
                self.semerror("__statement_result",token=stmt._stmt_token,\
                    message=message)
                return  # without adding the statement to the results
            self._stmt_counts[stmt.name]=counts+1
        except KeyError:
            # this means this is the first time for this statement, 
            # so it is always accepted
            self._stmt_counts[stmt.name]=1
        # Add the statement to the results.
        self._statements.append(stmt)
        
    # Report token errors from an xxxxx_eos list
    def __token_errors(self,who,eo_list):
        for x in eo_list:
            tok=x.token
            expected=x.expect
            message="%s expected '%s' but found: '%s'" \
                % (who,expected,tok.string)
            error=KWSemanticError("__token_errors",\
                type="T",token=tok,message=message)
            self.error(error)

    # Return the token type instance for a given TID or None if not registered.
    def _getToken(self,tid):
        try:
            return self._tokens[tid]
        except KeyError:
            # By returning None the better error messages can be supplied
            return None 

    # Overrides lang.Language default analyze method
    def analyze(self,text,sdebug=False):
        self.gs=self.lang.analyze(text,depth=20,lines=True,fail=False)
        if sdebug:
            self.lang.tokens()
        return self._statements   # Return the results

    # Called when a statement's keyword argument's value is about to be recognized.
    def cb_argument_beg(self,gs,pid):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_argument_beg(gs,pid='%s')" % pid)

        self.__reset_argument(gs)
     
    # Called when a statment's keyword argument's value has been recognized.
    def cb_argument_token(self,gs,pid,n,tok):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_argument_token"
                "(gs,pid='%s',n=%s,tok=%s)" % (pid,n,tok))

        arg_tok=gs.argument_token
        if arg_tok is not None:
            raise ValueError("langutil.py - KWLang.cb_argument_token() - "
                "'gs.argument_token' already set: %s" % arg_tok)
        gs.argument_token=tok
        
    # This method is called when a token fails recognition with an ErrorToken
    # instance
    def cb_argument_error(self,gs,pid,n,eo):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_argument_error"
                "(gs,pid='%s',n=%s,eo=%s)" % (pid,n,eo))

        self.__recognize_new_token_errors(gs.argument_eos,[eo,])
        return True
        
    # This method is called each time a keyword argument alternative fails.
    def cb_argument_failing(self,gs,pid,n,eo=[],last=False):
        if self.lang.isdebug("cbtrace"):
           print("langutil.py - KWLang.cb_argument_failing"
                "(gs,pid='%s',n=%s,eo=%s,last=%s)" % (pid,n,eo,last))
        self.__recognize_new_token_errors(gs.argument_eos,eo)
        return True
    
    # Called when recognition of a statement's keyword argument's value has
    # completed.  Failure implies the arguments value was not recognized.
    def cb_argument_end(self,gs,pid,failed=False,eo=[]):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_argument_end"
                "(gs,pid='%s',failed=%s,eo=%s)" % (pid,failed,eo))

        if failed:
            for x in gs.argument_eos:
                who="'%s' statement argument '%s' value" \
                    % (gs.stmt_def.name,gs.keyword)
                self.__token_errors(who,x)
            return self.__reset_argument(gs)
        gs.keyword_argument=gs.argument_token
        return self.__reset_argument(gs)

    # Called when attempts to recognize a statement's keyword argument occurs.
    def cb_keyword_beg(self,gs,pid):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_keyword_beg(gs,pid='%s')" % pid)

        self.__reset_keyword(gs)
        gs.keyword=pid
        gs.kwo=gs.stmt_def.keywordsd[pid]
    
    # Called first when the keyword argument name has been recognized
    # Called second when the EQUAL separator has been recognized
    # Note: the argument itself is calls self.cb_argument_xxxx() method
    def cb_keyword_token(self,gs,pid,n,tok):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_keyword_token"
                "(gs,pid='%s',n=%s,tok=%s)" % (pid,n,tok))

        state=gs.keyword_state
        if state == 0:  # First time, this is the keyword's token
            gs.keyword_token=tok
        elif state == 1:   # Second time, this is the EQUAL token (ignored)
            pass
        else:
            raise ValueError("langutil.py - KWLang.cb_keyword_token() - "
                "'gs.keyword_state' invalid value, not 0, 1, or 2: %s" % state)
        gs.keyword_state+=1
    
    # This method is called when a token fails recognition with an ErrorToken
    # instance
    def cb_keyword_error(self,gs,pid,n,eo):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_keyword_error"
                "(gs,pid='%s',n=%s,eo=%s)" % (pid,n,eo))

        state=gs.keyword_state
        if state == 0:  # First time, this is the keyword's token
            self.__recognize_new_token_errors(gs.keyword_eos,[eo,])
        elif state == 1:   # Second time, this is the EQUAL token (ignored)
            errors=self.__find_token_errors([eo,])
            gs.keyword_equal_eos.extend(errors)
        else:
            raise ValueError("langutil.py - KWLang.cb_keyword_failing() - "
                "'gs.keyword_state' invalid value, not 0, 1, or 2: %s" % state)
        if self.lang.isdebug("edebug"):
            print("langutil.py - KWLang.cb_keyword_error() - state=%s, "
                "gs.keyword_equal_eos: %s" % (state,gs.keyword_equal_eos))
            print("langutil.py - KWLang.cb_keyword_error() - state=%s, "
                "gs.keyword_eos: %s" % (state,gs.keyword_eos))
        return True

    # This method is called each time a keyword argument alternative fails.
    def cb_keyword_failing(self,gs,pid,n,eo=[],last=False):
        if self.lang.isdebug("cbtrace"):
           print("langutil.py - KWLang.cb_keyword_failing"
                "(gs,pid='%s',n=%s,eo=%s,last=%s)" % (pid,n,eo,last))
        self.__recognize_new_token_errors(gs.keyword_eos,eo)
        return True

    # Called at the completion of the recogniztion of the keyword sequence
    def cb_keyword_end(self,gs,pid,failed=False,eo=[]):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_keyword_end"
                "(gs,pid='%s',failed=%s,eo=%s)" % (pid,failed,eo))

        if failed:
            if self.lang.isdebug("edebug"):
                print("langutil.py - KWLang.cb_keyword_end() - "
                    "gs.keyword_equal_eos: %s" % gs.keyword_equal_eos)
                print("langutil.py - KWLang.cb_keyword_end() - "
                    "gs.keyword_eos: %s" % gs.keyword_eos)
            for x in gs.keyword_equal_eos:
                who="'%s' statement argument '%s'" \
                    % (gs.stmt_def.name,gs.keyword)
                self.__token_errors(who,gs.keyword_equal_eos)
 
            who="'%s' statement" % gs.stmt_def.name 
            self.__token_errors(who,gs.keyword_eos)
            return self.__reset_keyword(gs)

        # At this point tokens for both the keyword and its argument have been
        # recognized.
        error=gs.stmt_def._set_keyword(gs.statement,gs.keyword,gs.keyword_token,\
            gs.keyword_argument)
        if error:
            self.semerror(token=gs.keyword_token,\
                message="'%s' argument '%s' may only be specified once" \
                % (gs.statement.name,gs.keyword))
        return self.__reset_keyword(gs)

    # Called when a statment's positional arguments are starting to
    # recognized
    def cb_positinal_beg(self,gs,pid):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_positional_beg(gs,pid='%s')" % pid)

        self.__reset_positional(gs)
     
    # Called when a statment's positional argument has been recognized.
    def cb_positional_token(self,gs,pid,n,tok):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_positional_token"
                "(gs,pid='%s',n=%s,tok=%s)" % (pid,n,tok))

        gs.positionals.append(tok)
        
    # This method is called when a token fails recognition with an ErrorToken
    # instance
    def cb_positional_error(self,gs,pid,n,eo):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_positional_error"
                "(gs,pid='%s',n=%s,eo=%s)" % (pid,n,eo))
        gs.positional_eos.append(eo)

    # Called when recognition of a statement's positional arguments has completed.  
    # Failure implies a problem with the positional arguments.
    def cb_positional_end(self,gs,pid,failed=False,eo=[]):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_positional_end"
                "(gs,pid='%s',failed=%s,eo=%s)" % (pid,failed,eo))

        defn=gs.stmt_def
        num_req=defn.num_pos
        pos_num=len(gs.positionals)
        if failed:
            for x in gs.positional_eos:
                gs.mgr.report(x)
            self.semerror(token=gs.statement_token,\
                message="'%s' statement %s of %s required positional arguments "
                "missing" \
                % (gs.statement.name,num_req-pos_num,num_req))
            self.__reset_positional(gs)
            return False
        if num_req!=pos_num:
            raise ValueError("langutil.py - KWLang,cb_positional_end() - "
                "internal error: expected %s positional aguments encountered: %s" \
                % (num_req,pos_num))

        defn._set_positionals(gs.statement,gs.positionals)
        return False  # production errors should be reported

    # Called when a recognition begins with the start production
    def cb_start_beg(self,gs,pid):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_start_beg(gs,pid='%s')" % pid)
        # Initialize the global scope

        gs.statements=[]   # List of recognized KWStatement instances
        
        # See cb_statement_beg/end
        gs.stmt_def=None   # Current KWDef object of statement being recognized
        gs.statement=None  # Current KWStatement object being built.
    
    # Called at completion of the recognition process
    def cb_start_end(self,gs,pid,failed=False,eo=[]):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_start_end"
                "(gs,pid='%s',failed=%s,eo=%s)" % (pid,failed,eo))
        self.__required_statements()
        return True

    # Called when a trying to recognize a specifc statement
    def cb_statement_beg(self,gs,pid):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statement_beg(gs,pid='%s')" % pid)

        self.__reset_statement(gs)
        # the current KWDef object being recognized
        stmt=self._stmtsd[pid]
        gs.stmt_def=stmt
        # Results of the statement recognition process
        gs.statement=KWStatement(stmt)

    # Called when the statement's starting token has been recognized
    def cb_statement_token(self,gs,pid,n,tok):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statement_token"
                "gs,pid='%s',n=%s,tok=%s)" % (pid,n,tok))

        gs.statement_token=tok   # Saved for error reporting in global state
        gs.statement._set_stmt_tok(tok)
        
    # This method is called when a token fails recognition with an ErrorToken
    # instance
    def cb_statement_error(self,gs,pid,n,eo):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statement_error"
                "(gs,pid='%s',n=%s,eo=%s)" % (pid,n,eo))

        self.__recognize_new_token_errors(gs.statement_eos,[eo,])
        return True

    # This method is called each time a statement alternative fails.
    def cb_statement_failing(self,gs,pid,n,eo=[],last=False):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statement_failing"
                "gs,pid='%s',n=%s,eo=%s,last=%s)" % (pid,n,eo,last))

        self.__recognize_new_token_errors(gs.statement_eos,eo)
        return True

    # Called when the statement recognition process has completed.
    # Failure implies the statement was not recognized.
    def cb_statement_end(self,gs,pid,failed=False,eo=[]):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statement_end"
                "gs,pid='%s',failed=%s,eo=%s)" % (pid,failed,eo))

        if failed:
            if self.lang.isdebug("edebug"):
                print("langutil.py - KWLang.cb_statement_end() - "
                    "gs.statement_eos: %s" % gs.statement_eos)
            self.__token_errors("statement",gs.statement_eos)
            return self.__reset_statement(gs)

        statement=gs.statement   # Save KWstatement instance
        kwdef=gs.stmt_def        # Save the KWDef instance
        
        # Set any default keyword value for those that have them
        # Receive a list of required keyword arguments missing.
        required=kwdef._set_default(statement)
        
        # If any, report missing required keyword arguments
        if len(required)!=0:
            stmt_tok=gs.statement_token  # Use the statement token for errors
            stmt=kwdef.name
            for x in required:
                self.semerror(token=stmt_tok,message="'%s' statement '%s' "
                    "missing required keyword argument '%s'" % (stmt,x))
            return self.__reset_statement(gs)
        
        # KWStatement instance will be now part of the result
        self.__statement_result(statement,gs.stmt_def.multiple)
        return self.__reset_statement(gs)

    # Called when a statement's keyword arguments are starting to be recognized
    # This method also indicates the end of any positional argyments in the
    # statement
    def cb_statement_key_beg(self,gs,pid):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statement_key_end(gs,pid='%s')" % pid)

    # Called when all keyword arguments have been recognized.
    def cb_statement_key_end(self,gs,pid,failed=False,eo=[]):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statement_key_end"
                "(gs,pid='%s',failed=%s,eo=%s)" % (pid,failed,eo))

        return True

    # Called when the end of all statement options has completed.
    # Note: there is no corrsponding '_beg' method when starting to recognize
    # a statement.
    def cb_statements_end(self,gs,pid,failed=False,eo=[]):
        if self.lang.isdebug("cbtrace"):
            print("langutil.py - KWLang.cb_statements_end(gs,pid='%s')" % pid)

        return True

    # Method supplied by subclass defining the keyword language and initilizing
    # the embedded parser.
    # Required by lang.Processor
    def configure(self,lang,debug=False,cbdebug=False,gdebug=False,edebug=False):
        
        lang.flag("cbtrace")          # Define my debug flag
        # These flags control debug information provided by MyLexer and 
        # the parser.Parser classes debug options
        if self.debug:
            lang.debug(pdebug=True)   # debug MyLanguage parser
            lang.debug(prdebug=True)  # debug my productions in the grammar
            lang.debug(ldebug=True)   # debug MyLexer when running
            
        if self.cbdebug:
            lang.debug(cbtrace=True)  # debug my call backs

        # These flags control debug information for Grammar and GLexer
        if self.gdebug:
            lang.debug(gdebug=True)   # debug the Grammar operation
            lang.debug(gtdebug=True)  # debug the GLexer
            lang.debug(gldebug=True)  # debug the Grammar lexical processing

        # Debug error generation
        if self.edebug:
            lang.debug(edebug=True)   # debug my errors
        
        # Establish generic call backs
        lang.cbreg("start","beg",self.cb_start_beg)
        lang.cbreg("start","end",self.cb_start_end)
        lang.cbreg("statements","end",self.cb_statements_end)
        for stmt in self._stmts:
            pid=stmt.pid
            lang.cbreg(pid,"beg",self.cb_statement_beg)
            lang.cbreg(pid,"token",self.cb_statement_token)
            lang.cbreg(pid,"error",self.cb_statement_error)
            lang.cbreg(pid,"failing",self.cb_statement_failing)
            lang.cbreg(pid,"end",self.cb_statement_end)
            lang.cbreg("%s_key" % pid,"beg",self.cb_statement_key_beg)
            lang.cbreg("%s_key" % pid,"end",self.cb_statement_key_end)
        for x in self._pos_cbs:
            lang.cbreg(x,"beg",self.cb_positinal_beg)
            lang.cbreg(x,"token",self.cb_positional_token)
            lang.cbreg(x,"end",self.cb_positional_end)
        for x in self._kwd_cbs:
            lang.cbreg(x,"beg",self.cb_keyword_beg)
            lang.cbreg(x,"token",self.cb_keyword_token)
            lang.cbreg(x,"error",self.cb_keyword_error)
            lang.cbreg(x,"failing",self.cb_keyword_failing)
            lang.cbreg(x,"end",self.cb_keyword_end)
        for x in self._kwd_arg_cbs:
            lang.cbreg(x,"beg",self.cb_argument_beg)
            lang.cbreg(x,"token",self.cb_argument_token)
            lang.cbreg(x,"error",self.cb_argument_error)
            lang.cbreg(x,"failing",self.cb_argument_failing)
            lang.cbreg(x,"end",self.cb_argument_end)

    # Required by lang.Processor - provide the lexical analyzer
    def create_lexer(self):
        return self._lexer

    # Required by lang.Processor - provides the grammar and starting production
    def define_parser(self):
        return (self._grammar,"start")

    # Filter the input tokens for just the ones I need
    # Overrides parser.Parser.filter() default handling
    def filter(self,gs,tok):
        if tok.istype("unrecognized"):
            gs.mgr.report(parser.ErrorUnrecognized(tok))
            return None
        if tok.ignore:
            return None
        return tok
    
    # Initializes the language processing components
    def init_lang(self,debug=False,cbdebug=False,edebug=False,gdebug=False,
                  kdebug=False,tdebug=False):
        self.debug=debug
        self.cbdebug=cbdebug
        self.edebug=edebug
        self.gdebug=gdebug
        self.kdebup=kdebug
        self.tdebug=tdebug
        self.__init_lexer(kdebug=kdebug,tdebug=tdebug)
        self.__init_grammar(gdebug=gdebug)
        # Now initialize the parser, process the grammer and make it ready to use
        self.init()  # Instantiate the language.
        if kdebug:
            self.lang.grammar()
            self.lang.productions()

    # Print accumulated errors with help of the error manager
    def report(self):
        mgr=self.lang.gs.mgr
        lst=mgr.present()
        if len(lst)==0:
            print("no errors found")
        else:
            print("Errors found: %s" % len(lst))
        debug=self.lang.isdebug("edebug")
        for x in lst:
            x.print(debug=debug)

    # Report a semantic error to the error manager
    # Method arguments:
    #    source    A string identifying where the error was generated.
    #    token     The lexer.Token associated with the error.  Used to identify
    #              where in the source text the error occurred and influences
    #              where the error is reported to the user.
    def semerror(self,source,type="S",token=None,message=""):
        eo=KWSemanticError(source,type=type,token=token,message=message)
        self.error(eo)

    # Define a statement recognized by the language processor
    def statement(self,stmt):
        if not isinstance(stmt,KWDef):
            raise ValueError("langutil.py - KWLang.statement() - 'stmt' must be an "
                "instance of KWDef: %s" % stmt)
        stmt._validate(self)  # validate and convert TID's to lexer.Token instances
        pid=stmt.pid
        try:
            s=self._stmtsd[pid]
            raise ValueError("langutil.py - KWLang.statement() - duplicate "
                "statement PID: %s" % pid)
        except KeyError:
            self._stmtsd[pid]=stmt
            self._stmts.append(stmt)
            self._stmts_pids.append(pid)
            self._stmts_sync.append(stmt.tid)

    # Register a token type of interest to the language processor
    def token(self,tok):
        if not isinstance(tok,lexer.Type):
            raise ValueError("langutil.py - KWLang.token() - 'tok' must be an "
                "instance of lexer.Type: %s" % tok)
        if not tok.tid.isupper():
            raise ValueError("langutil.py - KWLang.token() - 'tok' TID must be "
                "upper case: %s" % tok.tid)
        self._lexlist.append(tok)
        self._tokens[tok.tid]=tok

class KWSemanticError(parser.Error):
    def __init__(self,source,type="S",token=None,message=""):
        self.type=type
        self.message=message
        if token is None:
            super().__init__(line=None,pos=None,source=source)
        else:
            super().__init__(line=token.line,pos=token.linepos,source=source)
        
    def print(self,debug=False):
        print("%s%s%s" % (self.type,self.loc(),self.message))

# Insances of this class represent the result of successful statement recognizion
# A list of instances of this class is returned by the KWLang.analyze() method.
class KWStatement(object):
    def __init__(self,defn):
        if not isinstance(defn,KWDef):
            raise ValueError("langutil.py - KWStatement - 'defn' argument must "
                "be a instance of KWDef: %s" % defn)
        self.name=defn.name  # Name of this statement
        self._def=defn       # KWDef instance associated with the statement
        self._pos_values=[]  # Positional values
        self._key_values={}  # Keyword values
        
        # These are used to position semantic error reporting within the input
        # stream.
        self._stmt_token=None   # The token that start statement recognition
        self._pos_tokens=[]     # Positional value source tokens
        self._key_tokens={}     # Keyword source tokens
        self._arg_tokens={}     # Argument source tokens
        
    # Extract argument value.  If key is a string, a key-word argument value
    # is returned.  Otherwise the key is assumed to be an integer and a positional
    # argument is returned.  This method allows use of indexing and key-based
    # extraction of statement arguments.  If s is an object instance of 
    # KWStatement, then
    #    s["string"]   Returns the key-word statement argument's value.  If the
    #                  argument "string" is not present in the statement, a
    #                  KeyError exception is raised.
    #    s[integer]    Returns the corresponding positional argument's value.  If
    #                  no positional parameter exists at the specified position,
    #                  an IndexError exceptions is raised.
    def __getitem__(self,key):
        if isinstance(key,str):
            return self._key_values[key]
        if isinstance(key,int):
            return self._pos_values[key]
        raise ValueError("langutil.py - KWStatement.__getitem__() - item key "
            "is not a string or integer: %s" % key)

    def __setitem__(self,key):
        if isinstance(key,str):
            arg="keyword"
            key="'%s'" % key
        elif isinstance(key,int):
            arg="positional"
            key="%s" % key
        else:
            raise ValueError("langutil.py - KWStatement.__setitem__() - item key "
                "is not a string or integer: %s" % key)
        raise NotImplementedError("langutil.py - KWStatement.__setitem__() - "
            "statement arguments are read-only, can not set %s argument: %s" \
            % (arg,key))

    # Provide a printable view of the statement
    def __str__(self):
        string="\n'%s' Statement" % self.name
        if len(self._pos_values)>0:
            pos=[]
            for x in self._pos_values:
                if isinstance(x,str):
                    pos.append("'%s'" % x)
                else:
                    pos.append("%s" % x)
            string="%s\nPOSITIONAL: %s" % (string,", ".join(pos))
        if len(self._key_values)>0:
            kwds=[]
            for key in self._key_values:
                kwds.append(key)
            kwds=sorted(kwds)
            string="%s\nKEYWORDS:" % string
            for key in kwds:
                values=self._key_values[key]
                if isinstance(values,str):
                    string="%s\n   %s='%s'" % (string,key,values)
                else:
                    string="%s\n   %s=%s" % (string,key,values)
        return string
        
    # This method is used internally to add a one of multiple occurences of
    # a keyword allowed to occur more than one time.
    def __add_keyword(self,key,arg,key_tok,arg_tok):
        try:
            key_values=self._key_values[key]
            key_tokens=self._key_tokens[key]
            arg_tokens=self._arg_tokens[key]
        except KeyError:
            key_values=[]
            key_tokens=[]
            arg_tokens=[]
        key_values.append(arg)
        key_tokens.append(key_tok)
        arg_tokens.append(arg_tok)
        self._key_values[key]=key_values
        self._key_tokens[key]=key_tokens
        self._arg_tokens[key]=arg_tokens
        
    # This method is used internally to establish default values for keywords
    # that are defined with a default value.  This method has the same semantics
    # as the dictionary method setdefault().
    def _set_default(self,keyword,default):
        self._key_values.setdefault(keyword,default)
        
    # Establishes the keyword and its argument's value and saves recognizing tokens.
    # tokens.  The caller must handle the situation when a duplicate keyword
    # argument occurs for a keyword not defined to occur multiple times.
    # Returns:
    #    True    If able to set the keyword
    #    False   If unable to set the keyword (because of duplicate)
    def _set_keyword(self,key,arg,key_tok,arg_tok,lst=False):
        # Keyword is defined to allow multiple occurences
        if lst:
            self.__add_keyword(key,arg,key_tok,arg_tok)
            return True
        try:
            key_values=self._key_values[key]
            return False
        except KeyError:
            pass
        self._key_values[key]=arg
        self._key_tokens[key]=key_tok
        self._arg_tokens[key]=arg_tok
        return True
        
    # Sets the positional argument values and their recognizing tokens.
    def _set_positionals(self,values,tokens):
        self._pos_values=values
        self._pos_tokens=tokens
        
    # Sets the recognizing token of the statement 
    def _set_stmt_tok(self,tok):
        if not isinstance(tok,lexer.Token):
            raise ValueError("langutil.py - KWStatement._set_stmt_tok() - "
                "'tok' argument must be an instance of lexer.Token: %s " % tok)
        self._stmt_token=tok
        
    # Returns an iterable dictionay view object of the keyword values
    def keywords(self):
        return self._key_values.keys()
        
    def positionals(self):
        return self._pos_values    # Return the list of positional arguments

if __name__ == "__main__":
    raise NotImplementedError("langutil.py - must only be imported")
